model:
  arch: st_llm_hf
  model_type: instructblip_vicuna0
  use_grad_checkpoint: True
  max_txt_len: 256
  end_sym: "###"
  video_input: "all"
  llama_model: '/path/to/lmsys/vicuna-7b-v1.1'
  ckpt: '/path/to/lmsys/instruct_blip_vicuna7b_trimmed.pth'
  q_former_model: '/path/to/lmsys/instruct_blip_vicuna7b_trimmed.pth'
  qformer_text_input: True
  freeze_LLM: False
  use_mask : False
  mvm_decode: False
  use_stg_after_vitln: True
  stg_num_layers: 3
  stg_is_gating: True
  stg_use_mlp_swiglu: True
  stg_use_temporal_attn_rope: True
  stg_use_spatial_attn_rope: True
  stg_use_rope_qkv_bias: True
  stg_use_rope_qkv_norm: True
  random_seed: 42
  
datasets:
  classification_k710_new:
    num_frames: 16
  classification_ssv2_new:
    num_frames: 16
  reasoning_next_qa_new:
    num_frames: 16
  reasoning_clevrer_qa_new:
    num_frames: 16
  reasoning_clevrer_mc_new:
    num_frames: 16

run:
  task: video_text_it
  bf16: True
  tf32: False
  output_dir: "./output/TG-Vid-197K"
  num_train_epochs: 2
  dataloader_num_workers: 4
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 1
  evaluation_strategy: "no"
  learning_rate: 2e-5
  weight_decay: 0.
  warmup_ratio: 0.03
  lr_scheduler_type: 'cosine'
  logging_steps: 50
  model_max_length: 1024
  #save_steps: 10000 
  save_strategy: "epoch" 
  save_total_limit: 1
  deepspeed: 'stllm/train/zero3.json'
  seed: 42